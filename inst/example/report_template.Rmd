# Time Series Analysis of Air Pollution and Health Data

```{r start time, echo=FALSE}
begin.time <- Sys.time()
```

## Inputs

```{r specify input parameters, child="input_parameters.Rmd", echo=TRUE}
```

```{r set global options, echo=FALSE}
opts_chunk$set(echo = FALSE)
```

## Initialization

Loading required packages. 
```{r loading packages, echo=TRUE, warning=FALSE, message=FALSE}
library(tsModel)
library(moments)
library(xlsx)
library(splines)
library(DSM)
library(digest)
```

## Read in Data

We read in the raw data from the file `r sQuote(datafile)`.

```{r reading data file}
## This function reads in .rds, .csv, or .xlsx data; returns an object
## of class 'APTSData'
d0 <- readAPTSData(datafile, response = response, exposure = exposure, timevar = timevar)
```

```{r hashing data file and data frame}
## Computing SHA-1 hashes for data file and initial data frame
hashfile <- digest(datafile, "sha1", file = TRUE)
hashdframe <- digest(d0, "sha1")
```

```{r summarize input data frame, echo=FALSE}
if(ncol(d0) < 20) {
        outputdesc <- paste("The variables in the data frame are named:", paste(sQuote(names(d0)), collapse = ", "))
} else {
        outputdesc <- paste("The first 20 variables in the data frame are named:", paste(sQuote(names(d0)[1:20]), collapse = ", "))
}
```

## Exploratory Data Analysis

The input data frame had `r nrow(d0)` rows and `r ncol(d0)` columns. `r outputdesc`. The response variable was `r sQuote(d0@response)` and the exposure variable was `r sQuote(d0@exposure)`. 


```{r exploratory data analysis}
nms <- names(d0)

## Check missing data
miss.exp <- mean(is.na(d0[, d0@exposure])) * 100
miss.response <- mean(is.na(d0[, d0@response])) * 100
comp <- mean(complete.cases(d0)) * 100
miss <- c(response = miss.response, exposure = miss.exp,
          complete = comp)

## Right skewness in predictors
use <- setdiff(nms, d0@response)
skewvar <- lapply(use, function(vname) {
        try(agostino.test(d0[, vname], alt = "less"), silent = TRUE)
})
u <- !sapply(skewvar, inherits, what = "try-error")
skewvar <- skewvar[u]
names(skewvar) <- use[u]
pv <- sapply(skewvar, "[[", "p.value")
pvadj <- pv[!is.na(pv)]
eda.summ <- list(miss = miss, skew = names(which(pvadj < 0.05)))
```

We conducted an exploratory analysis of the data. The median of `r sQuote(d0@response)` was `r median(d0[, d0@response], na.rm = TRUE)` with an inter-quartile range of `r IQR(d0[, d0@response], na.rm = TRUE)`. The median for the exposure `r sQuote(d0@exposure)` was `r median(d0[, d0@exposure], na.rm = TRUE)` with an inter-quartile range of `r IQR(d0[, d0@exposure], na.rm = TRUE)`.

For the response variable, `r if(miss.response == 0) { "none of the observations were missing" } else if(miss.response < 1) { "less than 1% of the observations were missing" } else { sprintf("%.1f of the observations were missing", miss.response) }`. For the exposure variable, `r if(miss.exp == 0) { "none of the observations were missing" } else if(miss.exp < 1) { "less than 1% of the observations were missing" } else { sprintf("%.1f%% of the observations were missing", miss.exp) }`.

`r if(length(eda.summ$skew) > 0L) { sprintf("Tests for skewness identified possible right skewness in the following variables: %s", sQuote(paste(eda.summ$skew, collapse = ", "))) }`.

Below is a histogram of the response variable `r sQuote(d0@response)`.

```{r response histogram}
yname <- d0@response
hist(d0[, yname], main = "Response", xlab = yname)
```

Below is a histogram of the exposure variable, `r sQuote(d0@exposure)`.

```{r exposure histogram}
xname <- d0@exposure
hist(d0[, xname], main = "Exposure", xlab = xname)
```

Below is a bivariate plot of response versus the exposure.

```{r response exposure scatterplot}
plot(d0[, xname], d0[, yname], xlab = xname, ylab = yname, pch = 20)
```

## Model Building

We fit a base model to the dataset with 

* The exposure variable `r sQuote(d0@exposure)` at lag `r exposure.lag`.

* 6 degrees of freedom per year for the smooth function of time.

* 6 degrees of freedom for a smooth function of same-day temperature

* 6 degrees of freedom for a smooth function of the running mean of temperature at lags 1 through 3

* 3 degrees of freedom for a smooth function of same-day dew point temperature

* 3 degrees of freedom for a smooth function of the running mean of dew point temperature at lags 1 through 3

* a categorical variable indicating the day of the week.

For all of the spline smoothers we use natural cubic splines for the basis functions.

```{r base model,echo=FALSE}
nyr <- nyears(d0[, d0@timevar])  ## Compute number of years in the dataset
basecall <- substitute(glm(death ~ dow + ns(date, nyr * 6) + ns(tmpd, 6) + ns(runMean(tmpd, 1:3), 6) + ns(dptp, 3) + ns(runMean(dptp, 1:3), 3) + Lag(pm10tmean, exposure.lag), data = as(d0, "data.frame"), family = poisson), list(nyr = nyr, exposure.lag = exposure.lag))
base <- eval(basecall)
print(base$call)
```

A key aspect of the model building process involves selecting the optimal degrees of freedom for the smooth function of time to adjust for unmeasured confounding. The approach is based on the methods described in Dominici et al. (2004) and Peng et al. (2006). Essentially, we choose the degrees of freedom per year that optimally predicts the variation in the exposure variable (`r sQuote(d0@exposure)`). The fit of the model to the data is measured via AIC and the range of degrees of freedom per year that we search over is between 2 and 20 per year. 

```{r selecting df per year, cache=TRUE}
dfseq <- 2:20
dat <- as(d0, "data.frame")
nyr <- nyears(dat[, d0@timevar])
fnames <- attr(terms(formula(base)), "term.labels")
tpos <- grep(d0@timevar, fnames)
epos <- grep(d0@exposure, fnames)
dfseq.total <- dfseq * nyr
timeVec <- paste0("ns(", d0@timevar, ", ", dfseq.total, ")")
results <- lapply(timeVec, function(tv) {
        newFormula <- reformulate(c(tv, fnames[-c(tpos, epos)]),
                                  response = d0@exposure)
        update(base, formula = newFormula, family = gaussian)
})
aic <- sapply(results, function(x) AIC(x))
names(aic) <- dfseq
dfmin <- dfseq[which.min(aic)]
```

The optimal number of degrees of freedom per year for the smooth
function of time is estimated to be `r dfmin`.

Below is a plot showing the AIC values versus the number of degrees of freedom per year in the smooth function of time.

```{r, plotting AIC values}
plot(dfseq, aic, xlab = "Degrees of freedom per year", ylab = "AIC", pch = 20)
```

## Model Fitting

With the selected degrees of freedom per year, we fit the final model to the data.

```{r fitting final model,echo=FALSE}
nyr <- nyears(d0[, d0@timevar])
finalexpr <- substitute(glm(death ~ dow + ns(date, nyr * dfmin) + ns(tmpd, 6) + ns(runMean(tmpd, 1:3), 6) + ns(dptp, 3) + ns(runMean(dptp, 1:3), 3) + Lag(pm10tmean, exposure.lag), data = as(d0, "data.frame"), family = poisson), list(nyr = nyr, dfmin = as.numeric(dfmin), exposure.lag = exposure.lag))
final <- eval(finalexpr)
print(final$call)
```

## Results
```{r}
varname <- grep(d0@exposure, attr(terms(formula(final)), "term.labels"), value = TRUE)
summ.final <- summary(final)
cf <- summ.final$coefficients[varname, 1]
pval <- summ.final$coefficients[varname, 4]
ci <- confint.default(final, varname)
p.incr <- 100 * (exp(10 * cf) - 1)
p.ci <- 100 * (exp(10 * ci) - 1)
```

We found that a 10 unit increase in the exposure `r sQuote(d0@exposure)` was associated with a `r sprintf("%s (95%% CI: %s, %s)", prettyNum(p.incr), prettyNum(p.ci[1]), prettyNum(p.ci[2]))` percent increase in the response `r sQuote(d0@response)`. This result was `r if(pval >= 0.05) { "not" }` statistically significant at the level 0.05 (p = `r prettyNum(pval)`).

## Sensitivity Analysis

Run sensitivity analysis with respect to the degrees of freedom for
the smooth function of time.

```{r sensitivity analysis, cache=TRUE}
dfseq <- as.integer(dfseq)
dfseq.total <- dfseq * nyears(d0)
fnames <- attr(terms(formula(base)), "term.labels")
timevar <- d0@timevar
pos <- grep(timevar, fnames)
origTimeVar <- fnames[pos]
timeVec <- paste("ns(", timevar, ", ", dfseq.total, ")", sep = "")
results <- vector("list", length = length(dfseq.total))

for(i in seq(along = dfseq.total)) {
        newFormula <- reformulate(c(fnames[-pos], timeVec[i]),
                                  response = d0@response)
        results[[i]] <- update(final, formula = newFormula)
}
names(results) <- as.character(dfseq)
```

```{r plot sensitivity analysis, fig.width=10}
exposureTerm <- grep(d0@exposure, fnames, value = TRUE)
b <- sapply(results, function(x) coef(x)[exposureTerm])
ci <- sapply(results, function(x) confint.default(x, exposureTerm))
xpts <- seq_along(results)
ypts <- b
plot(xpts, ypts, ylim = range(ci), xlab = "Models (df/year)", ylab = "Coefficient", pch = 19, xaxt = "n", main = "Sensitivity Analysis")
segments(xpts, ci[1, ], xpts, ci[2, ])
abline(h = coef(final)[exposureTerm], lty = 2)
abline(h = 0, lty = 3, col = "red")
legend("topright", legend = "Final Estimate", lty = 2,
       bty = "n")
axis(1, xpts, names(results))
```

After fitting the final model, we examine the leverages and Cook's distance to identify potentially influential points that may affect the estimation of the exposure regression coefficient.

```{r find possible influential points based on final model}
n <- summary(final)$df.null + 1
lev <- hatvalues(final)
cdist <- cooks.distance(final) * n
ulev <- lev > mean(lev) * 4
eda.summ$highlev <- names(which(ulev))
ucd <- cdist > qchisq(0.99, 1) & rank(cdist) > (n - 10)
eda.summ$outlier <- names(which(ucd))
infl <- with(eda.summ, unique(c(highlev, outlier)))
```

In this dataset we identified `r length(infl)` potentially influential points, given the final fitted model. `r if(length(infl) > 0L) { "Below we plot the exposure regression coefficient after removing each influential point individually and after removing all of them simultaneously (the x-axis shows the observation row labels from the original data frame)" }`.

```{r influential points analysis,cache=TRUE}
if(length(infl) > 0L) {
        dat <- as(d0, "data.frame")
        modellist <- lapply(infl, function(id) {
                i <- match(id, row.names(dat))
                newdat <- dat[-i, ]
                m <- update(final, data = newdat)
                })
        i <- match(infl, row.names(dat))
        rm.all <- update(final, data = dat[-i, ])
        results <- append(modellist, list(rm.all))
        names(results) <- c(infl, "all")
}
```

```{r plot influental points anlaysis, fig.width=10}
if(length(infl) > 0L) {
        ## Plot results
        exposureTerm <- grep(d0@exposure, fnames, value = TRUE)
        b <- sapply(results, function(x) coef(x)[exposureTerm])
        ci <- sapply(results, function(x) confint.default(x, exposureTerm))
        xpts <- seq_along(results)
        ypts <- b
        plot(xpts, ypts, ylim = range(ci), xlab = "Influential Points", ylab = "Coefficient", pch = 19, xaxt = "n", main = "Influential Points Analysis")
        segments(xpts, ci[1, ], xpts, ci[2, ])
        abline(h = coef(final)[exposureTerm], lty = 2)
        abline(h = 0, lty = 3, col = "red")
        legend("topright", legend = "Final Estimate", lty = 2,
               bty = "n")
        axis(1, xpts, names(results))
}
```

## Outputs

```{r final output objects, echo=FALSE}
out.objects = c("final", "response", "exposure", "exposure.lag", "timevar")
outputfile <- sprintf("%s-output.rda", datafile)
save(list = out.objects, file = outputfile, compress = "xz")
dg <- digest(outputfile, "sha1", file = TRUE)
```

The final output is stored in the file **`r outputfile`** which contains the objects: `r sQuote(out.objects)`.

The SHA-1 hash for the output file is: `r dg`.

## Appendix

### Summary of the final fitted model.

```{r final model summary}
summary(final)
```

### R session information

```{r session info}
sessionInfo()
```

SHA-1 hashes for the input data file and the original data frame.

```{r hash information, echo=FALSE}
cat(sprintf("%s: %s\n", sQuote(datafile), hashfile))
cat(sprintf("data frame: %s\n", hashdframe))
```


This analysis started on `r format(begin.time)` and was completed on
`r format(Sys.time())`.
